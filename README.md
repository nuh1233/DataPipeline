# DataPipeline

A flexible, configuration-driven ETL (Extract, Transform, Load) pipeline for processing tabular data. Built with Python, this pipeline supports multiple file formats and provides powerful data transformation capabilities with minimal code changes.

## âœ¨ Features

- **Multi-format Support**: Auto-detects and processes CSV, Parquet, JSON, and Excel files
- **Configuration-Driven**: Define multiple pipeline configurations via JSON without modifying code
- **Flexible Transformations**:
  - Filter or keep rows based on column values
  - Custom sorting with user-defined category orders
  - Clustering and grouping with statistical summaries
- **Multiple Output Formats**: Export to CSV or Parquet with optional compression (snappy, gzip)
- **Statistics Generation**: Optional per-cluster and sub-cluster statistics

## ğŸ“‹ Prerequisites

- Python 3.7+
- pip (Python package installer)

## ğŸš€ Quick Start

### Installation

```bat
cd big-data\data_pipeline
python -m venv ..\venv
..\venv\Scripts\python -m pip install -r ..\requirements.txt
```

### Usage

List all available pipeline configurations:
```bat
cd big-data\data_pipeline
..\venv\Scripts\python main.py list
```

Run a specific pipeline configuration:
```bat
..\venv\Scripts\python main.py nyc_buildings
```

## ğŸ“ Project Structure

```
DataPipeline/
â”œâ”€â”€ main.py                  # Entry point and CLI
â”œâ”€â”€ data_processing.py       # Core ETL logic
â”œâ”€â”€ datasets_config.json     # Pipeline configurations
â”œâ”€â”€ Nyc_data.csv            # Sample input data
â”œâ”€â”€ output/                 # Generated output files
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Pipeline configurations are defined in `datasets_config.json`. Each configuration supports:

| Parameter | Description | Required |
|-----------|-------------|----------|
| `input_file` | Source data file path | âœ“ |
| `output_file` | Output file name (CSV or Parquet) | âœ“ |
| `output_dir` | Output directory path | âœ“ |
| `filter_column` | Column to filter rows from | |
| `filter_values` | Values to exclude | |
| `keep_column` | Column to keep specific rows | |
| `keep_values` | Values to retain | |
| `primary_column` | Main grouping column | |
| `sub_columns` | Sub-grouping columns | |
| `sort_order` | Custom category order | |
| `compression` | Compression type (snappy, gzip) | |
| `show_stats` | Generate statistics (true/false) | |

### Example Configuration

```json
{
  "manhattan_only": {
    "input_file": "Nyc_data.csv",
    "output_file": "Manhattan_buildings.parquet",
    "output_dir": "output/nyc_green_data",
    "compression": "gzip",
    "keep_column": "City",
    "keep_values": ["Manhattan"],
    "primary_column": "Largest Property Use Type",
    "show_stats": true
  }
}
```

## ğŸ“Š Example Pipelines

The repository includes several pre-configured pipelines for NYC buildings data:

- `nyc_buildings`: Filter parking properties with custom city sorting
- `nyc_no_parking_storage`: Exclude parking and storage properties
- `nyc_multifamily_only`: Keep only multifamily housing and offices
- `property_types_parquet`: Export specific property types to compressed Parquet
- `manhattan_only`: Extract Manhattan buildings with statistics

## ğŸ”§ ETL Process Flow

```
Extract â†’ Transform â†’ Load
   â†“          â†“          â†“
Auto-detect â†’ Filter   â†’ CSV/Parquet
  format   â†’ Sort      â†’ with optional
           â†’ Cluster   â†’ compression
           â†’ Stats
```

## ğŸ“ Notes

- Place your input data file (`Nyc_data.csv` or custom files) in the project root
- Output files are automatically created in the specified `output_dir`
- The pipeline creates directories if they don't exist

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the issues page.

## ğŸ‘¤ Author

**nuh1233**

- GitHub: [@nuh1233](https://github.com/nuh1233)
